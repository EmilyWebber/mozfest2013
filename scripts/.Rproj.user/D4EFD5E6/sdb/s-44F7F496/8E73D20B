{
    "contents" : "# Sentence-level text analysis \n\n## Sentiment analysis in R\n\nWe're going to take advantage of a library that comes with a pretrained model, Tim Jurka's [sentiment](https://github.com/timjurka/sentiment) package. It's a perfectly suitable Naive Bayes classifier, but it is rather slow.  I don't recommend it for large datasets (read: millions of sentences).\n\nSentence-level annotations presumably translates well to other short forms of texts, such as microblogging (Twitter, Facebook statuses, etc).\n\nWe're going to use a very small random sample of the very famous `sentiment polarity dataset v1.0` by Pang and Lee, available [here](http://www.cs.cornell.edu/people/pabo/movie-review-data/).  This consists of thousands of processed sentences/snippets of movie reviews from the Rotten Tomatoes website.\n\n```{r warning=FALSE}\nlibrary(plyr)\nlibrary(reshape2)\nlibrary(sentiment)\n```\n\n\n# Sentiment\n```{r warning=FALSE}\n#Using Tim Jurka's sentiment package\n#Code adapted from Collingwood and Jurka\nsetwd(\"/Users/rweiss/Dropbox/presentations/MozFest2013/data/\")\nrt_neg = read.delim(\"rt-polaritydata/rt-polaritydata/rt-polarity.neg\", header=F, quote=\"\")\nrt_pos = read.delim(\"rt-polaritydata/rt-polaritydata/rt-polarity.pos\", header=F, quote=\"\")\nrt_neg = data.frame(rt_neg)\nrt_pos = data.frame(rt_pos)\nrt_neg$label = \"negative\"\nrt_pos$label = \"positive\"\nreviews = rbind(rt_neg, rt_pos)\nnames(reviews) = c('content','label')\n\n#let's start off with a small sample\nsample_size = 100\nnum_documents = dim(reviews)[1]\nreviews_sample <- reviews[sample(1:num_documents,size=sample_size,replace=FALSE),]\nreviews_sample$content = as.character(reviews_sample$content)\n\nfoo = data.frame(as.character(reviews_sample$label), as.character(reviews_sample$content))\nwrite.csv(reviews_sample$content, '../data/reviews_data.csv', row.names=F)\n#let's save this so we can compare it against the SASA tool\nwrite.table(foo, '../sentiment_examples/reviews_sample.csv', col.names=c('label','content'), row.names=F, quote=F, sep=\"\\t\")\n\nclass(reviews_sample) #make sure it is a data frame object\nhead(reviews_sample) # Look at the first six lines or so\nsummary(reviews_sample) #summarize the data\nsapply(reviews_sample, class) #look at the class of each column\ndim(reviews_sample) #Check the dimensions, rows and columns\n\npredicted_sentiment = ddply(reviews_sample, .(content), function(x){\n  classify_polarity(x, algorithm=\"bayes\")\n})\n\ntable(reviews_sample$label)\n\npredicted_sentiment$\"POS/NEG\" = as.numeric(as.character(predicted_sentiment$\"POS/NEG\"))\npredicted_sentiment$label = cut(predicted_sentiment$\"POS/NEG\", breaks=c(0, 1, max(predicted_sentiment$\"POS/NEG\")))\nlevels(predicted_sentiment$label)  = c('negative','positive')\nnumber_correct = sum(predicted_sentiment$label == reviews_sample$label)\nnumber_correct / sample_size\n\ntable(reviews_sample$label)\ntable(predicted_sentiment$label)\nreviews_labeled = data.frame(predicted_sentiment$label, predicted_sentiment$content)\nwrite.csv(reviews_labeled, '../sentiment_examples/reviews_sample_2.csv', row.names=F)\n```\n\n## Big picture questions:\n1. These models all have baseline accuracies measured against very famous, annotated datasets.  Therefore we have an estimate of how \"accurate\" a model should be. \n2. Go through the resulting predicted sentiment labels and examine whether you agree or disagree with them.  Count the proportion of values you agree with and then compare your agreement ratio agains the measured baseline accuracy.  How similar is it?\n3. How appropriate is this model for this kind of data?  \n4. What was this model trained on?  \n5. How similar is the language of that training data against this movie review data?\n6. How do these results compare against the other model's results?\n7. We created a very simple bipolar classification.  By default, SASA will do positive, negative, neutral, and unsure.  Other models will do 5pt classification (very positive-very negative).  Still others will do discrete, categorical sentiment (see Wiebe's subjectivity lexicon).  There are many, many ways to label sentiment.  Which do you prefer?\n",
    "created" : 1382732358808.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2124110282",
    "id" : "8E73D20B",
    "lastKnownWriteTime" : 1382732915,
    "path" : "~/Dropbox/presentations/MozFest2013/scripts/R_sentiment.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}
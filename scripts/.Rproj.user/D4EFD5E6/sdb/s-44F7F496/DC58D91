{
    "contents" : "# Document-level text analysis\n\nDocument-level analysis is when you are interested in the whole text article, not tokens (sentences or words).  The most basic example is labeling documents against some classification scheme, hence **text classification**.  When you don't know your scheme ahead of time or you're interested in exploring a large set of data, you can try **topic modeling**.\n\nWe're going to go over a couple of examples of document-level text analysis using some very most common classifiers models.  We're going to go over the code to train your own model and discuss the results we see.\n\n## Supervised learning: Text classification in R\n\nWe're going to go over examples of how to use the excellent [RTextTools](http://www.rtexttools.com/) library to train some text classifiers.\n\nThe dataset used are the titles and topic codes from the `NYTimes` dataset that comes with the RTextTools library in `R`.  It consists of titles from NYTimes front page news and associated codes according to [Amber Boydstun's classification scheme](http://www.policyagendas.org/sites/policyagendas.org/files/Boydstun_NYT_FrontPage_Codebook_0.pdf).\n\n```{r warning=FALSE,echo=FALSE,results='hide'}\nlibrary(RTextTools)\n```\n\n```{r warning=FALSE}\n# Code adapted from Collingwood and Jurka\n\n# READ THE CSV DATA from the RTextTools package\n# Note that RTextTools has many dependencies, but Collingwood & Jurka [wisely] chose to keep all the dependencies R-friendly (read, no Java that I know of).\ndata(NYTimes)\n\n#there isn't that much data in this dataset for training\n#so we're going to subset down only to those data that contain a lot of observations\ntable(NYTimes$Topic.Code)\n#consider only using 3, 12, 15, 16, 19, 20, 29\nvalid = c(3, 12, 15, 16, 19, 20, 29)\nNYTimes = NYTimes[NYTimes$Topic.Code %in% valid,]\ntable(NYTimes$Topic.Code)\nnum_documents = dim(NYTimes)[1]\n\n#Examine the data\nclass(NYTimes) #make sure it is a data frame object\nhead(NYTimes) # Look at the first six lines or so\nsummary(NYTimes) #summarize the data\nsapply(NYTimes, class) #look at the class of each column\ndim(NYTimes) #Check the dimensions, rows and columns\n\n# [OPTIONAL] SUBSET YOUR DATA TO GET A RANDOM SAMPLE\n# we don't have that much data, so we're going to keep it all\n# sample_size = 500\nsample_size = num_documents\nNYT_sample <- NYTimes[sample(1:num_documents,size=sample_size,replace=FALSE),]\n\nout_data = data.frame(NYT_sample$Topic.Code, NYT_sample$Title)\nwrite.csv(out_data, 'nyt_title_data.csv', row.names=F)\n\n# CREATE A TERM-DOCUMENT MATRIX THAT REPRESENTS WORD FREQUENCIES IN EACH DOCUMENT\n# WE WILL TRAIN ON THE Title COLUMNS\n#NYT_dtm <- create_matrix(data.frame(NYT_sample$Title,NYT_sample$Subject), \nNYT_dtm <- create_matrix(as.vector(NYT_sample$Title), \n                         language=\"english\", \n                         removeNumbers=TRUE, \n                         stemWords=TRUE, \n                         weighting=weightTfIdf)\n\nNYT_dtm # Sparse Matrix object\n\n# CORPUS AND CONTAINER CREATION \n\n#choosing the right size of training/test data is a personal decision\n#let's go with an 80/20 split; this is quite common\ntrain_n = round(sample_size * 0.8)\ntest_n = round(sample_size * 0.2)\n\ncorpus <- create_container(NYT_dtm,\n                        NYT_sample$Topic.Code,\n                        trainSize=1:train_n,\n                        testSize=(train_n+1):sample_size,\n                        virgin=FALSE)\n\nnames(attributes(corpus))\npaste(NYT_sample[1,]$Title) # original data\ncorpus@column_names[corpus@training_matrix[1]@ja] # preprocessed data\n\n# TRAIN MODELS\nmodels <- train_models(corpus, algorithms=c(\"SVM\",\"MAXENT\"))\nresults <- classify_models(corpus, models)\nanalytics <- create_analytics(corpus, results)\n\nnyt_codes = read.csv(\"../data/nytimes_codes.csv\")\ntest_start_index = num_documents - train_n\nsvm_full = data.frame(NYT_sample[1730:2161,]$Title, results$SVM_LABEL)\nmaxent_full = data.frame(NYT_sample[1730:2161,]$Title, results$MAXENTROPY_LABEL)\n\nnames(svm_full) = c('content','code')\nnames(maxent_full) = c('content','code')\nsvm_full= merge(svm_full, nyt_codes)\nmaxent_full= merge(maxent_full, nyt_codes)\n\n#lets take a random sample of each of these and ask people to verify the coding\n\nsvm_mozfest = svm_full[sample(1:432,size=100,replace=FALSE),]\nmaxent_mozfest = maxent_full[sample(1:432,size=100,replace=FALSE),]\n\nwrite.csv(svm_mozfest, '../labeling_examples/svm_mozfest.csv', row.names=FALSE)\nwrite.csv(maxent_mozfest,'../labeling_examples/maxent_mozfest.csv', row.names=FALSE)\n```\n\n\n```{r warning=FALSE}\n#SUMMARY OF PRECISION, RECALL, F-SCORES, AND ACCURACY SORTED BY TOPIC CODE FOR EACH ALGORITHM\nanalytics@algorithm_summary\n# SUMMARY OF LABEL (e.g. TOPIC) ACCURACY\n#analytics@label_summary\n# RAW SUMMARY OF ALL DATA AND SCORING\n#analytics@document_summary\n```\n\n```{r warning=FALSE, fig.show='asis'}\nx <- as.character(rownames(analytics@algorithm_summary))[-20]\ny <- analytics@algorithm_summary$SVM_RECALL[-20]\nplot(x, y, type=\"l\", lwd=3, main=\"Support Vector Machine Topic Accuracy\", ylab=\"Recall Accuracy\", xlab=\"Topic\")\nabline(h=.75, lwd=2, col=\"maroon\")\ntext(x, y, adj=1.2)\n\nx <- as.character(rownames(analytics@algorithm_summary))[-20]\ny <- analytics@algorithm_summary$MAXENTROPY_RECALL[-20]\nplot(x, y, type=\"l\", lwd=3, main=\"Maximum Entropy Topic Accuracy\", ylab=\"Recall Accuracy\", xlab=\"Topic\")\nabline(h=.75, lwd=2, col=\"maroon\")\ntext(x, y, adj=1.2)\n```\n\n## Unsupervised learning: topic modeling\n\n```{r warning=FALSE}\nlibrary(topicmodels)\n\n#term frequency vectors, not tf-idf vectors\nn_topics = 20\nNYT_dtm <- create_matrix(as.vector(NYT_sample$Title), \n                         language=\"english\", \n                         removeNumbers=FALSE, \n                         stemWords=FALSE, #only because they are short \n                         weighting=weightTf)\n\nrowTotals <- apply(NYT_dtm , 1, sum)\nNYT_dtm_full   <- NYT_dtm[rowTotals> 0]  \n\nk <- length(unique(NYT_sample$Topic.Code))\nlda <- LDA(NYT_dtm_full, k)\n\n\ndata(\"AssociatedPress\", package = \"topicmodels\")\nlda <- LDA(AssociatedPress[1:1000,], control = list(alpha = 0.1), k = 20)\nlda_inf <- posterior(lda, AssociatedPress[21:30,])\n```\n",
    "created" : 1382700177842.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1185420412",
    "id" : "DC58D91",
    "lastKnownWriteTime" : 1382699328,
    "path" : "~/Dropbox/presentations/MozFest2013/scripts/R_classification.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}
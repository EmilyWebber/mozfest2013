# Document-level text analysis

Document-level analysis is when you are interested in the whole text article, not tokens (sentences or words).  The most basic example is labeling documents against some classification scheme, hence **text classification**.  When you don't know your scheme ahead of time or you're interested in exploring a large set of data, you can try **topic modeling**.

We're going to go over a couple of examples of document-level text analysis using some very most common classifiers models.  We're going to go over the code to train your own model and discuss the results we see.

## Supervised learning: Text classification in R

We're going to go over examples of how to use the excellent [RTextTools](http://www.rtexttools.com/) library to train some text classifiers.

The dataset used are the titles and topic codes from the `NYTimes` dataset that comes with the RTextTools library in `R`.  It consists of titles from NYTimes front page news and associated codes according to [Amber Boydstun's classification scheme](http://www.policyagendas.org/sites/policyagendas.org/files/Boydstun_NYT_FrontPage_Codebook_0.pdf).

```{r warning=FALSE,echo=FALSE,results='hide'}
library(RTextTools)
```

```{r warning=FALSE}
# Code adapted from Collingwood and Jurka
# see here: http://www.rtexttools.com/1/post/2012/02/rtexttools-short-course-materials.html
# READ THE CSV DATA from the RTextTools package
# Note that RTextTools has many dependencies, but Collingwood & Jurka [wisely] chose to keep all the dependencies R-friendly (read, no Java that I know of).
data(NYTimes)

#there isn't that much data in this dataset for training
#so we're going to subset down only to those data that contain a lot of observations
table(NYTimes$Topic.Code)
#consider only using 3, 12, 15, 16, 19, 20, 29
valid = c(3, 12, 15, 16, 19, 20, 29)
NYTimes = NYTimes[NYTimes$Topic.Code %in% valid,]
table(NYTimes$Topic.Code)
num_documents = dim(NYTimes)[1]

#Examine the data
class(NYTimes) #make sure it is a data frame object
head(NYTimes) # Look at the first six lines or so
summary(NYTimes) #summarize the data
sapply(NYTimes, class) #look at the class of each column
dim(NYTimes) #Check the dimensions, rows and columns

# [OPTIONAL] SUBSET YOUR DATA TO GET A RANDOM SAMPLE
# we don't have that much data, so we're going to keep it all
# sample_size = 500
sample_size = num_documents
NYT_sample = NYTimes[sample(1:num_documents,size=sample_size,replace=FALSE),]

out_data = data.frame(NYT_sample$Topic.Code, NYT_sample$Title)
write.csv(out_data, '../data/nyt_title_data.csv', row.names=F)

# CREATE A TERM-DOCUMENT MATRIX THAT REPRESENTS WORD FREQUENCIES IN EACH DOCUMENT
# WE WILL TRAIN ON THE Title COLUMNS
#NYT_dtm = create_matrix(data.frame(NYT_sample$Title,NYT_sample$Subject), 
NYT_dtm = create_matrix(as.vector(NYT_sample$Title), 
                         language="english", 
                         removeNumbers=TRUE, 
                         stemWords=TRUE, 
                         weighting=weightTfIdf)

NYT_dtm # Sparse Matrix object

# CORPUS AND CONTAINER CREATION 

#choosing the right size of training/test data is a personal decision
#let's go with an 80/20 split; this is quite common
train_n = round(sample_size * 0.8)
test_n = round(sample_size * 0.2)

corpus = create_container(NYT_dtm,
                        NYT_sample$Topic.Code,
                        trainSize=1:train_n,
                        testSize=(train_n+1):sample_size,
                        virgin=FALSE)

names(attributes(corpus))
paste(NYT_sample[1,]$Title) # original data
corpus@column_names[corpus@training_matrix[1]@ja] # preprocessed data

# TRAIN MODELS
models = train_models(corpus, algorithms=c("SVM","MAXENT"))
results = classify_models(corpus, models)
analytics = create_analytics(corpus, results)

nyt_codes = read.csv("../data/nytimes_codes.csv")
test_start_index = num_documents - train_n
svm_full = data.frame(NYT_sample[1730:2161,]$Title, results$SVM_LABEL)
maxent_full = data.frame(NYT_sample[1730:2161,]$Title, results$MAXENTROPY_LABEL)

names(svm_full) = c('content','code')
names(maxent_full) = c('content','code')
svm_full= merge(svm_full, nyt_codes)
maxent_full= merge(maxent_full, nyt_codes)

#lets take a random sample of each of these and ask people to verify the coding

svm_mozfest = svm_full[sample(1:432,size=100,replace=FALSE),]
maxent_mozfest = maxent_full[sample(1:432,size=100,replace=FALSE),]

write.csv(svm_mozfest, '../labeling_examples/svm_mozfest.csv', row.names=FALSE)
write.csv(maxent_mozfest,'../labeling_examples/maxent_mozfest.csv', row.names=FALSE)
```


```{r warning=FALSE}
#SUMMARY OF PRECISION, RECALL, F-SCORES, AND ACCURACY SORTED BY TOPIC CODE FOR EACH ALGORITHM
analytics@algorithm_summary
# SUMMARY OF LABEL (e.g. TOPIC) ACCURACY
#analytics@label_summary
# RAW SUMMARY OF ALL DATA AND SCORING
#analytics@document_summary
```

```{r warning=FALSE, fig.show='asis'}
x = as.character(rownames(analytics@algorithm_summary))[-20]
y = analytics@algorithm_summary$SVM_RECALL[-20]
plot(x, y, type="l", lwd=3, main="Support Vector Machine Topic Accuracy", ylab="Recall Accuracy", xlab="Topic")
abline(h=.75, lwd=2, col="maroon")
text(x, y, adj=1.2)

x = as.character(rownames(analytics@algorithm_summary))[-20]
y = analytics@algorithm_summary$MAXENTROPY_RECALL[-20]
plot(x, y, type="l", lwd=3, main="Maximum Entropy Topic Accuracy", ylab="Recall Accuracy", xlab="Topic")
abline(h=.75, lwd=2, col="maroon")
text(x, y, adj=1.2)
```

## Unsupervised learning: topic modeling

```{r warning=FALSE}
library(topicmodels)

#term frequency vectors, not tf-idf vectors
n_topics = 60
NYT_dtm = create_matrix(as.vector(NYT_sample$Title), 
                         language="english", 
                         removeNumbers=FALSE, 
                         stemWords=FALSE, #only because they are short 
                         weighting=weightTf)

rowTotals = apply(NYT_dtm , 1, sum)
NYT_dtm_full  = NYT_dtm[rowTotals> 0]  

#k = length(unique(NYT_sample$Topic.Code))
lda = LDA(NYT_dtm_full, n_topics)

topic = topics(lda, 1)
topic[1]

terms = terms(lda, 10)
terms

#data("AssociatedPress", package = "topicmodels")
#lda = LDA(AssociatedPress[1:1000,], control = list(alpha = 0.1), k = 20)
#lda_inf = posterior(lda, AssociatedPress[21:30,])
```

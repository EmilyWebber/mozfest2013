{
 "metadata": {
  "name": "Python_examples.ipynb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import csv\n",
      "#os.chdir('/path/to/wherever/you/downloaded/data/from/textcleaning')\n",
      "os.chdir('/Users/rweiss/Dropbox/presentations/MozFest2013/data/')\n",
      "\n",
      "review_data = []\n",
      "labels = set()\n",
      "tsvfile = open('reviews_sample.csv', 'r')\n",
      "csv_reader = csv.DictReader(tsvfile, delimiter=',')\n",
      "for line in csv_reader:\n",
      "    temp = {line['label'] : line['content']}\n",
      "    review_data.append(temp)\n",
      "    labels.add(line['label'])\n",
      "tsvfile.close()\n",
      "\n",
      "print 'There are ' + str(len(review_data)) + ' total reviews.'\n",
      "\n",
      "print 'The labels are '+ ', '.join(labels) + '.'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 100 total reviews.\n",
        "The labels are positive, negative.\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from random import shuffle\n",
      "x = [review_data[i] for i in range(len(review_data))]\n",
      "shuffle(x)\n",
      "#let's subset down to 500 tweets\n",
      "#x = x[0:500]\n",
      "\n",
      "review_data = [''.join(el.values()) for el in x] \n",
      "target_labels = [''.join(el.keys()) for el in x]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Sentence-level"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Named Entity Recognition\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "ner_path = \"/Users/rweiss/Dropbox/presentations/MozFest2013/vendor/stanford-ner-2013-06-20\"\n",
      "infile = open(\"nyt_titles.csv\", 'r')\n",
      "nyt_titles = []\n",
      "for line in infile:\n",
      "    nyt_titles.append(line)\n",
      "\n",
      "from nltk.tag.stanford import NERTagger\n",
      "st = NERTagger(os.path.join(ner_path, 'classifiers/english.all.3class.distsim.crf.ser.gz'),\n",
      "                os.path.join(ner_path, 'stanford-ner.jar'))\n",
      "\n",
      "for title in nyt_titles[0:20]:\n",
      "    print st.tag(title.split()) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('``', 'O'), ('Nation', 'O'), (\"'s\", 'O'), ('Smaller', 'O'), ('Jails', 'O'), ('Struggle', 'O'), ('To', 'O'), ('Cope', 'O'), ('With', 'O'), ('Surge', 'O'), ('in', 'O'), ('Inmates', 'O'), (\"''\", 'O')]\n",
        "[('``', 'O'), ('FEDERAL', 'O'), ('IMPASSE', 'O'), ('SADDLING', 'O'), ('STATES', 'O'), ('WITH', 'O'), ('INDECISION', 'O'), (\"''\", 'O')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[('``', 'O'), ('Long', 'O'), (',', 'O'), ('Costly', 'O'), ('Prelude', 'O'), ('Does', 'O'), ('Little', 'O'), ('To', 'O'), ('Alter', 'O'), ('Plot', 'O'), ('of', 'O'), ('Presidential', 'O'), ('Race', 'O'), (\"''\", 'O')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[('``', 'O'), ('Top', 'O'), ('Leader', 'O'), ('of', 'O'), ('the', 'O'), ('Bosnian', 'O'), ('Serbs', 'O'), ('Now', 'O'), ('Under', 'O'), ('Attack', 'O'), ('From', 'O'), ('Within', 'O'), (\"''\", 'O')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[('``', 'O'), ('BATTLE', 'O'), ('OVER', 'O'), ('THE', 'O'), ('BUDGET', 'O'), (':', 'O'), ('THE', 'O'), ('OVERVIEW', 'O'), (';', 'O'), ('LEADERS', 'O'), ('IN', 'O'), ('HOUSE', 'O'), ('DROP', 'O'), ('G.O.P.', 'O'), ('PLAN', 'O'), ('ON', 'O'), ('U.S.', 'LOCATION'), ('WORKERS', 'O'), (\"''\", 'O')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[('``', 'O'), ('South', 'O'), ('African', 'O'), ('Democracy', 'O'), ('Stumbles', 'O'), ('on', 'O'), ('Old', 'O'), ('Rivalry', 'O'), (\"''\", 'O')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[('``', 'O'), ('Among', 'O'), ('Economists', 'O'), (',', 'O'), ('Little', 'O'), ('Fear', 'O'), ('on', 'O'), ('Deficit', 'O'), (\"''\", 'O')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[('``', 'O'), ('BATTLE', 'O'), ('OVER', 'O'), ('THE', 'O'), ('BUDGET', 'O'), (':', 'O'), ('THE', 'O'), ('OVERVIEW', 'O'), (';', 'O'), ('TALKS', 'O'), ('ON', 'O'), ('BUDGET', 'O'), ('ARE', 'O'), ('PUT', 'O'), ('ON', 'O'), ('HOLD', 'O'), ('AMID', 'O'), ('UNCERTAINTY', 'O'), (\"''\", 'O')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[('``', 'O'), ('High', 'O'), ('Court', 'O'), ('Is', 'O'), ('Cool', 'O'), ('To', 'O'), ('Census', 'O'), ('Change', 'O'), (\"''\", 'O')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[('``', 'O'), ('TURMOIL', 'O'), ('AT', 'O'), ('BARNEYS', 'O'), (':', 'O'), ('THE', 'O'), ('DIFFICULTIES', 'O'), (';', 'O'), ('Barneys', 'ORGANIZATION'), ('Is', 'O'), ('Seeking', 'O'), ('Bankruptcy', 'O'), (',', 'O'), ('Citing', 'O'), ('Fight', 'O'), ('With', 'O'), ('Partner', 'O'), (\"''\", 'O')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[('``', 'O'), ('Clinton', 'PERSON'), ('Visits', 'O'), ('Bosnia', 'LOCATION'), ('to', 'O'), ('Thank', 'O'), ('the', 'O'), ('G.I.', 'O'), (\"'s\", 'O'), (\"''\", 'O')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[('``', 'O'), ('Managed', 'O'), ('Care', 'O'), ('Has', 'O'), ('Trouble', 'O'), ('Treating', 'O'), ('AIDS', 'O'), (',', 'O'), ('Patients', 'O'), ('Say', 'O'), (\"''\", 'O')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[('``', 'O'), ('Russians', 'O'), ('Pounding', 'O'), ('Rebels', 'O'), ('Who', 'O'), ('Hold', 'O'), ('100', 'O'), ('Hostages', 'O'), (\"''\", 'O')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[('``', 'O'), ('BUDGET', 'O'), ('STANDOFF', 'O'), ('DEEPENS', 'O'), ('AS', 'O'), ('G.O.P.', 'O'), ('CALLS', 'O'), ('OFF', 'O'), ('TALKS', 'O'), (\"''\", 'O')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[('``', 'O'), ('In', 'O'), ('Uneasy', 'O'), ('Time', 'O'), (',', 'O'), ('Saudi', 'O'), ('Prince', 'O'), ('Provides', 'O'), ('a', 'O'), ('Hope', 'O'), ('of', 'O'), ('Stability', 'O'), (\"''\", 'O')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[('``', 'O'), ('POLITICS', 'O'), (':', 'O'), ('IN', 'O'), ('THE', 'O'), ('SOUTH', 'O'), (';', 'O'), ('A', 'O'), ('Southern', 'O'), ('Democrat', 'O'), ('Resists', 'O'), ('The', 'O'), ('Lure', 'O'), ('of', 'O'), ('Party', 'O'), ('Switching', 'O'), (\"''\", 'O')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[('``', 'O'), ('With', 'O'), ('the', 'O'), ('Greatest', 'O'), ('of', 'O'), ('Ease', 'O'), ('...', 'O'), (\"''\", 'O')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[('``', 'O'), ('Supreme', 'O'), ('Court', 'O'), ('Roundup', 'O'), (';', 'O'), ('Christian', 'ORGANIZATION'), ('Scientists', 'ORGANIZATION'), ('Rebuffed', 'ORGANIZATION'), ('in', 'O'), ('Ruling', 'O'), ('By', 'O'), ('Supreme', 'ORGANIZATION'), ('Court', 'ORGANIZATION'), (\"''\", 'O')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[('``', 'O'), ('Newark', 'LOCATION'), ('Airport', 'LOCATION'), ('Is', 'O'), ('Pressing', 'O'), ('to', 'O'), ('Surpass', 'O'), ('Kennedy', 'O'), (\"''\", 'O')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[('``', 'O'), ('Giuliani', 'PERSON'), ('Weighs', 'O'), ('Reducing', 'O'), ('Police', 'O'), ('Force', 'O'), ('by', 'O'), ('1,000', 'O'), ('Jobs', 'O'), (\"''\", 'O')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Sentiment Analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/python\n",
      "import sys\n",
      "\n",
      "try:\n",
      "    from sasa.classifier import Classifier\n",
      "except ImportError:\n",
      "    raise ImportError(\"Did you try to run '. setup.env'?\\n\" + \n",
      "                      \"(or add the sasa-tool directory to PYTHONPATH, ie export PYTHONPATH=<this director>)?\")\n",
      "\n",
      "classifier = Classifier()    \n",
      "classified_reviews = []\n",
      "\n",
      "for line in review_data:\n",
      "    print \"classifying %s\" % line.strip()\n",
      "    sentiment, valence, posterior = classifier.classifyFromText(line)\n",
      "    if valence >= 0:\n",
      "        score = \"positive\"\n",
      "    elif valence <0:\n",
      "        score = \"negative\"\n",
      "    classified_reviews.append({score: line}) \n",
      "\n",
      "outfile = open('reviews_sample_3.tsv', 'w')\n",
      "\n",
      "for line in classified_reviews:\n",
      "    body = ''.join(line.values())\n",
      "    label = ''.join(line.keys())\n",
      "    body = unicode(body)\n",
      "    body = body.decode('utf-8')\n",
      "    body = body.encode('utf-8')\n",
      "    label = label.encode('utf-8')\n",
      "    try:\n",
      "        outfile.write(label + '\\t' + body + '\\n')\n",
      "    except UnicodeDecodeError:\n",
      "        print \"Unicode error\" + line\n",
      "outfile.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "classifying everyone connected to this movie seems to be part of an insider clique , which tends to breed formulaic films rather than fresh ones .\n",
        "classifying unless you are in dire need of a diesel fix , there is no real reason to see it . wait for video -- and then don't rent it .\n",
        "classifying an enjoyable above average summer diversion .\n",
        "classifying millions of dollars heaped upon a project of such vast proportions need to reap more rewards than spiffy bluescreen technique and stylish weaponry .\n",
        "classifying rosenthal ( halloween ii ) seems to have forgotten everything he ever knew about generating suspense .\n",
        "classifying by turns very dark and very funny .\n",
        "classifying a woefully dull , redundant concept that bears more than a whiff of exploitation , despite iwai's vaunted empathy .\n",
        "classifying ultimately , it ponders the reasons we need stories so much .\n",
        "classifying a little too pat for its own good .\n",
        "classifying a sleek advert for youthful anomie that never quite equals the sum of its pretensions .\n",
        "classifying a modestly comic , modestly action-oriented world war ii adventure that , in terms of authenticity , is one of those films that requires the enemy to never shoot straight .\n",
        "classifying it's like going to a house party and watching the host defend himself against a frothing ex-girlfriend . you don't want to call the cops . you want to call domino's .\n",
        "classifying an honest , sensitive story from a vietnamese point of view .\n",
        "classifying an awkwardly contrived exercise in magic realism .\n",
        "classifying a fine film , but it would be a lot better if it stuck to betty fisher and left out the other stories .\n",
        "classifying apparently kissing leads to suicide attempts and tragic deaths . marisa tomei is good , but just a kiss is just a mess .\n",
        "classifying like schindler's list , the grey zone attempts to be grandiloquent , but ends up merely pretentious -- in a grisly sort of way .\n",
        "classifying the movie's blatant derivativeness is one reason it's so lackluster .\n",
        "classifying boomers and their kids will have a barrie good time .\n",
        "classifying cox creates a fluid and mesmerizing sequence of images to match the words of nijinsky's diaries .\n",
        "classifying it's definitely a step in the right direction .\n",
        "classifying the superior plotline isn't quite enough to drag along the dead ( water ) weight of the other .\n",
        "classifying as quiet , patient and tenacious as mr . lopez himself , who approaches his difficult , endless work with remarkable serenity and discipline .\n",
        "classifying art-house to the core , read my lips is a genre-curling crime story that revives the free-wheeling noir spirit of old french cinema .\n",
        "classifying never engaging , utterly predictable and completely void of anything remotely interesting or suspenseful .\n",
        "classifying road to perdition does display greatness , and it's worth seeing . but it also comes with the laziness and arrogance of a thing that already knows it's won .\n",
        "classifying karmen moves like rhythm itself , her lips chanting to the beat , her long , braided hair doing little to wipe away the jeweled beads of sweat .\n",
        "classifying tsai ming-liang's witty , wistful new film , what time is it there ? , is a temporal inquiry that shoulders its philosophical burden lightly .\n",
        "classifying juwanna mann ?  no thanks . wewannour money back , actually .\n",
        "classifying the scorpion king is more fun than conan the barbarian .\n",
        "classifying most of the movie is so deadly dull that watching the proverbial paint dry would be a welcome improvement .\n",
        "classifying a breezy romantic comedy that has the punch of a good sitcom , while offering exceptionally well-detailed characters .\n",
        "classifying what happened with pluto nash ? how did it ever get made ?\n",
        "classifying deserves a place of honor next to nanook as a landmark in film history .\n",
        "classifying preposterous and tedious , sonny is spiked with unintentional laughter that , unfortunately , occurs too infrequently to make the film even a guilty pleasure .\n",
        "classifying the characters are interesting and the relationship between yosuke and saeko is worth watching as it develops , but there's not enough to the story to fill two hours .\n",
        "classifying the filmmakers want nothing else than to show us a good time , and in their cheap , b movie way , they succeed .\n",
        "classifying everything else about high crimes is , like the military system of justice it portrays , tiresomely regimented .\n",
        "classifying halfway through , however , having sucked dry the undead action flick formula , blade ii mutates into a gross-out monster movie with effects that are more silly than scary .\n",
        "classifying the most surprising thing about this film is that they are actually releasing it into theaters .\n",
        "classifying stanley kwan has directed not only one of the best gay love stories ever made , but one of the best love stories of any stripe .\n",
        "classifying schnitzler's film has a great hook , some clever bits and well-drawn , if standard issue , characters , but is still only partly satisfying .\n",
        "classifying the problems and characters it reveals are universal and involving , and the film itself -- as well its delightful cast -- is so breezy , pretty and gifted , it really won my heart .\n",
        "classifying how anyone over the age of 2 can stomach the touchy-feely message this preachy produce promotes is beyond us .\n",
        "classifying zhang . . . has done an amazing job of getting realistic performances from his mainly nonprofessional cast .\n",
        "classifying perhaps not since nelson eddy crooned his indian love call to jeanette macdonald has there been a movie so unabashedly canadian , not afraid to risk american scorn or disinterest .\n",
        "classifying a funny film .\n",
        "classifying if you dig on david mamet's mind tricks . . . rent this movie and enjoy !\n",
        "classifying [stephen] earnhart's film is more about the optimism of a group of people who are struggling to give themselves a better lot in life than the ones they currently have .\n",
        "classifying saved from being merely way-cool by a basic , credible compassion ."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "classifying rarely does a film so graceless and devoid of merit as this one come along .\n",
        "classifying i'll go out on a limb . it isn't quite one of the worst movies of the year . it's just merely very bad .\n",
        "classifying what a bewilderingly brilliant and entertaining movie this is .\n",
        "classifying while the film shuns the glamour or glitz that an american movie might demand , scherfig tosses us a romantic scenario that is just as simplistic as a hollywood production .\n",
        "classifying it's at once laughable and compulsively watchable , in its committed dumbness .\n",
        "classifying an entertaining , if somewhat standardized , action movie .\n",
        "classifying this tuxedo . . . should have been sent back to the tailor for some major alterations .\n",
        "classifying falters when it takes itself too seriously and when it depends too heavily on its otherwise talented cast to clown in situations that aren't funny .\n",
        "classifying what it lacks in originality it makes up for in effective if cheap moments of fright and dread .\n",
        "classifying there's something fishy about a seasonal holiday kids' movie . . . that derives its moment of most convincing emotional gravity from a scene where santa gives gifts to grownups .\n",
        "classifying wobbly senegalese updating of  carmen  which is best for the stunning star turn by djeinaba diop gai\n",
        "classifying after seeing swept away , i feel sorry for madonna .\n",
        "classifying intelligent and moving .\n",
        "classifying though only 60 minutes long , the film is packed with information and impressions .\n",
        "classifying the camera whirls ! the camera twirls ! oh , look at that clever angle ! wow , a jump cut !\n",
        "classifying even when he's not at his most critically insightful , godard can still be smarter than any 50 other filmmakers still at work .\n",
        "classifying a confident , richly acted , emotionally devastating piece of work and 2002's first great film\n",
        "classifying it's a sometimes interesting remake that doesn't compare to the brilliant original .\n",
        "classifying it's a lot to ask people to sit still for two hours and change watching such a character , especially when rendered in as flat and impassive a manner as phoenix's .\n",
        "classifying anyone who suffers through this film deserves , at the very least , a big box of consolation candy .\n",
        "classifying has a certain ghoulish fascination , and generates a fair amount of b-movie excitement .\n",
        "classifying mark pellington's latest pop thriller is as kooky and overeager as it is spooky and subtly in love with myth .\n",
        "classifying [it's] difficult to get beyond the overall blandness of american chai , despite its likable performances and refreshingly naive point of view .\n",
        "classifying i was sent a copyof this film to review on dvd . for free . i still want my money back .\n",
        "classifying greg kinnear gives a mesmerizing performance as a full-fledged sex addict who is in complete denial about his obsessive behavior .\n",
        "classifying the film jolts the laughs from the audience--as if by cattle prod .\n",
        "classifying nachtwey clears the cynicism right out of you . he makes you realize that deep inside righteousness can be found a tough beauty .\n",
        "classifying the film doesn't really care about the thousands of americans who die hideously , it cares about how ryan meets his future wife and makes his start at the cia .\n",
        "classifying for all its surface frenzy , high crimes should be charged with loitering -- so much on view , so little to offer .\n",
        "classifying the film is weighed down by supporting characters who are either too goodly , wise and knowing or downright comically evil .\n",
        "classifying sex ironically has little to do with the story , which becomes something about how lame it is to try and evade your responsibilities and that you should never , ever , leave a large dog alone with a toddler . but never mind all that ; the boobs are fantasti\n",
        "classifying it's quaid who anchors the film with his effortless performance and that trademark grin of his -- so perfect for a ballplayer .\n",
        "classifying a fascinating documentary that provides a rounded and revealing overview of this ancient holistic healing system\n",
        "classifying a winning comedy with its wry observations about long-lived friendships and the ways in which we all lose track of ourselves by trying to please others .\n",
        "classifying although it includes a fair share of dumb drug jokes and predictable slapstick ,  orange county  is far funnier than it would seem to have any right to be .\n",
        "classifying amid the shock and curiosity factors , the film is just a corny examination of a young actress trying to find her way .\n",
        "classifying arteta paints a picture of lives lived in a state of quiet desperation .\n",
        "classifying why come up with something even quasi-original , when you can pillage from shirley jackson , richard matheson . . . and puke up something like rose red ?\n",
        "classifying much of this slick and sprightly cgi feature is sufficiently funny to amuse even the most resolutely unreligious parents who escort their little ones to megaplex screenings .\n",
        "classifying when perry fists a bull at the moore farm , it's only a matter of time before he gets the upper hand in matters of the heart .\n",
        "classifying like the original , this version is raised a few notches above kiddie fantasy pablum by allen's astringent wit .\n",
        "classifying the bottom line with nemesis is the same as it has been with all the films in the series : fans will undoubtedly enjoy it , and the uncommitted needn't waste their time on it .\n",
        "classifying one well-timed explosion in a movie can be a knockout , but a hundred of them can be numbing . proof of this is ballistic : ecks vs . sever .\n",
        "classifying the writers , director wally wolodarsky , and all the actors should start their own coeducational fraternity : kappa rho alpha phi ."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "classifying jae-eun jeong's take care of my cat brings a beguiling freshness to a coming-of-age story with such a buoyant , expressive flow of images that it emerges as another key contribution to the flowering of the south korean cinema .\n",
        "classifying because the genre is well established , what makes the movie fresh is smart writing , skewed characters , and the title performance by kieran culkin .\n",
        "classifying eric byler's nuanced pic avoids easy sentiments and explanations . . .\n",
        "classifying the subtitled costume drama is set in a remote african empire before cell phones , guns , and the internal combustion engine , but the politics that thump through it are as timely as tomorrow .\n",
        "classifying wilco fans will have a great time , and the movie should win the band a few new converts , too .\n",
        "classifying nelson's intentions are good , but the end result does no justice to the story itself . it's horribly depressing and not very well done .\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Document-level"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Text classification"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn import metrics\n",
      "from operator import itemgetter\n",
      "from sklearn.metrics import classification_report\n",
      "import csv\n",
      "\n",
      "nyt = open('nyt_title_data.csv')\n",
      "nyt_data = []\n",
      "nyt_labels = []\n",
      "csv_reader = csv.reader(nyt)\n",
      "\n",
      "for line in csv_reader:\n",
      "    nyt_labels.append(int(line[0]))\n",
      "    nyt_data.append(line[1])\n",
      "\n",
      "nyt.close()\n",
      "\n",
      "trainset_size = int(round(len(nyt_data)*0.75)) # i chose this threshold arbitrarily...to discuss\n",
      "print 'The training set size for this classifier is ' + str(trainset_size) + '\\n'\n",
      "\n",
      "X_train = np.array([''.join(el) for el in nyt_data[0:trainset_size]])\n",
      "#y_train = np.array([''.join(el) for el in nyt_labels[0:trainset_size]])\n",
      "y_train = np.array([el for el in nyt_labels[0:trainset_size]])\n",
      "\n",
      "X_test = np.array([''.join(el) for el in nyt_data[trainset_size+1:len(nyt_data)]])   \n",
      "#y_test = np.array([''.join(el) for el in nyt_labels[trainset_size+1:len(nyt_labels)]])  \n",
      "y_test = np.array([el for el in nyt_labels[trainset_size+1:len(nyt_labels)]])  \n",
      "\n",
      "#print(X_train)\n",
      "\n",
      "vectorizer = TfidfVectorizer(min_df=2, \n",
      "    ngram_range=(1, 2), \n",
      "    stop_words='english', \n",
      "    strip_accents='unicode', \n",
      "    norm='l2')\n",
      "                             \n",
      "test_string = unicode(nyt_data[0])\n",
      "\n",
      "print \"Example string: \" + test_string\n",
      "print \"Preprocessed string: \" + vectorizer.build_preprocessor()(test_string)\n",
      "print \"Tokenized string:\" + str(vectorizer.build_tokenizer()(test_string))\n",
      "print \"N-gram data string:\" + str(vectorizer.build_analyzer()(test_string))\n",
      "print \"\\n\"\n",
      "    \n",
      "X_train = vectorizer.fit_transform(X_train)\n",
      "X_test = vectorizer.transform(X_test)\n",
      "\n",
      "nb_classifier = MultinomialNB().fit(X_train, y_train)\n",
      "\n",
      "y_nb_predicted = nb_classifier.predict(X_test)\n",
      "\n",
      "print \"MODEL: Multinomial Naive Bayes\\n\"\n",
      "\n",
      "print 'The precision for this classifier is ' + str(metrics.precision_score(y_test, y_nb_predicted))\n",
      "print 'The recall for this classifier is ' + str(metrics.recall_score(y_test, y_nb_predicted))\n",
      "print 'The f1 for this classifier is ' + str(metrics.f1_score(y_test, y_nb_predicted))\n",
      "print 'The accuracy for this classifier is ' + str(metrics.accuracy_score(y_test, y_nb_predicted))\n",
      "\n",
      "print '\\nHere is the classification report:'\n",
      "print classification_report(y_test, y_nb_predicted)\n",
      "\n",
      "#simple thing to do would be to up the n-grams to bigrams; try varying ngram_range from (1, 1) to (1, 2)\n",
      "#we could also modify the vectorizer to stem or lemmatize\n",
      "print '\\nHere is the confusion matrix:'\n",
      "print metrics.confusion_matrix(y_test, y_nb_predicted, labels=unique(nyt_labels))\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The training set size for this classifier is 1621\n",
        "\n",
        "Example string: Rivals Covet Giuliani's Crime Data, Not His Ways\n",
        "Preprocessed string: rivals covet giuliani's crime data, not his ways\n",
        "Tokenized string:[u'Rivals', u'Covet', u'Giuliani', u'Crime', u'Data', u'Not', u'His', u'Ways']\n",
        "N-gram data string:[u'rivals', u'covet', u'giuliani', u'crime', u'data', u'ways', u'rivals covet', u'covet giuliani', u'giuliani crime', u'crime data', u'data ways']\n",
        "\n",
        "\n",
        "MODEL: Multinomial Naive Bayes\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "The precision for this classifier is 0.6563301256\n",
        "The recall for this classifier is 0.508348794063\n",
        "The f1 for this classifier is 0.459605882312\n",
        "The accuracy for this classifier is 0.508348794063\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          3       0.89      0.17      0.29        46\n",
        "         12       0.67      0.04      0.08        49\n",
        "         15       1.00      0.17      0.30        46\n",
        "         16       0.53      0.55      0.54       103\n",
        "         19       0.41      0.88      0.56       152\n",
        "         20       0.74      0.60      0.67       101\n",
        "         29       1.00      0.12      0.21        42\n",
        "\n",
        "avg / total       0.66      0.51      0.46       539\n",
        "\n",
        "\n",
        "Here is the confusion matrix:\n",
        "[[  8   0   0   7  28   3   0]\n",
        " [  1   2   0   9  35   2   0]\n",
        " [  0   0   8   3  31   4   0]\n",
        " [  0   1   0  57  41   4   0]\n",
        " [  0   0   0  15 133   4   0]\n",
        " [  0   0   0   7  33  61   0]\n",
        " [  0   0   0   9  24   4   5]]\n"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#What are the top N most predictive features per class?\n",
      "N = 10\n",
      "vocabulary = np.array([t for t, i in sorted(vectorizer.vocabulary_.iteritems(), key=itemgetter(1))])\n",
      "\n",
      "for i, label in enumerate(nyt_labels):\n",
      "    if i == 7: # hack...\n",
      "        break\n",
      "    topN = np.argsort(nb_classifier.coef_[i])[-N:]\n",
      "    print \"\\nThe top %d most informative features for topic code %s: \\n%s\" % (N, label, \" \".join(vocabulary[topN]))\n",
      "    #print topN"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "The top 10 most informative features for topic code 12: \n",
        "drugs doctors hospitals medicare cancer health tobacco care new drug\n",
        "\n",
        "The top 10 most informative features for topic code 29: \n",
        "gun report death sniper suspect drug police new crime case\n",
        "\n",
        "The top 10 most informative features for topic code 16: \n",
        "ex market wall billion big stocks chief deal enron microsoft\n",
        "\n",
        "The top 10 most informative features for topic code 3: \n",
        "iraqi baghdad military 11 bush challenged nation challenged nation war iraq\n",
        "\n",
        "The top 10 most informative features for topic code 3: \n",
        "gaza overview israelis russia mideast leader india new israel china\n",
        "\n",
        "The top 10 most informative features for topic code 15: \n",
        "2000 campaign 2000 clinton politics testing testing president bush democrats campaign president\n",
        "\n",
        "The top 10 most informative features for topic code 19: \n",
        "season knicks mets world series world yankees playoffs game series baseball\n"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import LinearSVC\n",
      "\n",
      "svm_classifier = LinearSVC().fit(X_train, y_train)\n",
      "\n",
      "y_svm_predicted = svm_classifier.predict(X_test)\n",
      "print \"MODEL: Linear SVC\\n\"\n",
      "\n",
      "print 'The precision for this classifier is ' + str(metrics.precision_score(y_test, y_svm_predicted))\n",
      "print 'The recall for this classifier is ' + str(metrics.recall_score(y_test, y_svm_predicted))\n",
      "print 'The f1 for this classifier is ' + str(metrics.f1_score(y_test, y_svm_predicted))\n",
      "print 'The accuracy for this classifier is ' + str(metrics.accuracy_score(y_test, y_svm_predicted))\n",
      "\n",
      "print '\\nHere is the classification report:'\n",
      "print classification_report(y_test, y_svm_predicted)\n",
      "\n",
      "#simple thing to do would be to up the n-grams to bigrams; try varying ngram_range from (1, 1) to (1, 2)\n",
      "#we could also modify the vectorizer to stem or lemmatize\n",
      "print '\\nHere is the confusion matrix:'\n",
      "print metrics.confusion_matrix(y_test, y_svm_predicted, labels=unique(nyt_labels))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MODEL: Linear SVC\n",
        "\n",
        "The precision for this classifier is 0.634552115175\n",
        "The recall for this classifier is 0.614100185529\n",
        "The f1 for this classifier is 0.610677884775\n",
        "The accuracy for this classifier is 0.614100185529\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          3       0.61      0.48      0.54        46\n",
        "         12       0.43      0.33      0.37        49\n",
        "         15       0.71      0.52      0.60        46\n",
        "         16       0.57      0.61      0.59       103\n",
        "         19       0.55      0.77      0.64       152\n",
        "         20       0.78      0.68      0.73       101\n",
        "         29       0.91      0.48      0.62        42\n",
        "\n",
        "avg / total       0.63      0.61      0.61       539\n",
        "\n",
        "\n",
        "Here is the confusion matrix:\n",
        "[[ 22   3   1   5  11   4   0]\n",
        " [  7  16   2   4  16   3   1]\n",
        " [  0   5  24   5  11   1   0]\n",
        " [  0   4   1  63  29   6   0]\n",
        " [  5   3   3  19 117   4   1]\n",
        " [  2   4   1  10  15  69   0]\n",
        " [  0   2   2   4  13   1  20]]\n"
       ]
      }
     ],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#What are the top N most predictive features per class?\n",
      "N = 10\n",
      "vocabulary = np.array([t for t, i in sorted(vectorizer.vocabulary_.iteritems(), key=itemgetter(1))])\n",
      "\n",
      "for i, label in enumerate(nyt_labels):\n",
      "    if i == 7: # hack...\n",
      "        break\n",
      "    topN = np.argsort(svm_classifier.coef_[i])[-N:]\n",
      "    print \"\\nThe top %d most informative features for topic code %s: \\n%s\" % (N, label, \" \".join(vocabulary[topN]))\n",
      "    #print topN"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "The top 10 most informative features for topic code 12: \n",
        "patients driving drug cancer fat doctors care medicare hospitals tobacco\n",
        "\n",
        "The top 10 most informative features for topic code 29: \n",
        "murder guns fallen approves execution officer horror robbery crime gun\n",
        "\n",
        "The top 10 most informative features for topic code 16: \n",
        "houston merger response storm appears pricing profit stocks microsoft enron\n",
        "\n",
        "The top 10 most informative features for topic code 3: \n",
        "military abuse unit missile hussein arms rumsfeld nato 11 iraq\n",
        "\n",
        "The top 10 most informative features for topic code 3: \n",
        "india africa russia palestinians pakistan europe russian mideast chinese israel\n",
        "\n",
        "The top 10 most informative features for topic code 15: \n",
        "answer gingrich president impeachment clinton testing president lewinsky politics democrats campaign\n",
        "\n",
        "The top 10 most informative features for topic code 19: \n",
        "night worth match bowl playoffs yankees game mets series baseball\n"
       ]
      }
     ],
     "prompt_number": 75
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "maxent_classifier = LogisticRegression().fit(X_train, y_train)\n",
      "\n",
      "y_maxent_predicted = maxent_classifier.predict(X_test)\n",
      "print \"MODEL: Maximum Entropy\\n\"\n",
      "\n",
      "print 'The precision for this classifier is ' + str(metrics.precision_score(y_test, y_maxent_predicted))\n",
      "print 'The recall for this classifier is ' + str(metrics.recall_score(y_test, y_maxent_predicted))\n",
      "print 'The f1 for this classifier is ' + str(metrics.f1_score(y_test, y_maxent_predicted))\n",
      "print 'The accuracy for this classifier is ' + str(metrics.accuracy_score(y_test, y_maxent_predicted))\n",
      "\n",
      "print '\\nHere is the classification report:'\n",
      "print classification_report(y_test, y_maxent_predicted)\n",
      "\n",
      "#simple thing to do would be to up the n-grams to bigrams; try varying ngram_range from (1, 1) to (1, 2)\n",
      "#we could also modify the vectorizer to stem or lemmatize\n",
      "print '\\nHere is the confusion matrix:'\n",
      "print metrics.confusion_matrix(y_test, y_maxent_predicted, labels=unique(nyt_labels))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MODEL: Maximum Entropy\n",
        "\n",
        "The precision for this classifier is 0.677024456975\n",
        "The recall for this classifier is 0.576744186047\n",
        "The f1 for this classifier is 0.556394583721\n",
        "The accuracy for this classifier is 0.576744186047\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          3       1.00      0.24      0.38        17\n",
        "         12       0.60      0.20      0.30        15\n",
        "         15       0.88      0.35      0.50        20\n",
        "         16       0.63      0.52      0.57        50\n",
        "         19       0.45      0.86      0.59        63\n",
        "         20       0.76      0.74      0.75        35\n",
        "         29       1.00      0.27      0.42        15\n",
        "\n",
        "avg / total       0.68      0.58      0.56       215\n",
        "\n",
        "\n",
        "Here is the confusion matrix:\n",
        "[[ 4  1  0  2  9  1  0]\n",
        " [ 0  3  0  1 11  0  0]\n",
        " [ 0  0  7  2  9  2  0]\n",
        " [ 0  1  1 26 19  3  0]\n",
        " [ 0  0  0  8 54  1  0]\n",
        " [ 0  0  0  2  7 26  0]\n",
        " [ 0  0  0  0 10  1  4]]\n"
       ]
      }
     ],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#What are the top N most predictive features per class?\n",
      "N = 10\n",
      "vocabulary = np.array([t for t, i in sorted(vectorizer.vocabulary_.iteritems(), key=itemgetter(1))])\n",
      "\n",
      "for i, label in enumerate(nyt_labels):\n",
      "    if i == 7: # hack...\n",
      "        break\n",
      "    topN = np.argsort(maxent_classifier.coef_[i])[-N:]\n",
      "    print \"\\nThe top %d most informative features for topic code %s: \\n%s\" % (N, label, \" \".join(vocabulary[topN]))\n",
      "    #print topN"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IndexError",
       "evalue": "index 1783 is out of bounds for size 1659",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-76-2b208982f3d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtopN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxent_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"\\nThe top %d most informative features for topic code %s: \\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m#print topN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mIndexError\u001b[0m: index 1783 is out of bounds for size 1659"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Topic modeling"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim import corpora, models, similarities\n",
      "from itertools import chain\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "import re\n",
      "url_pattern = r'https?:\\/\\/(.*[\\r\\n]*)+'\n",
      "\n",
      "#documents = [el for el in nyt_data]\n",
      "#documents = list(chain.from_iterable(documents))\n",
      "#documents = [re.sub(url_pattern, '', doc, flags=re.MULTILINE) for doc in documents]\n",
      "documents = [nltk.clean_html(document) for document in nyt_data]\n",
      "stoplist = stopwords.words('english')\n",
      "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
      "          for document in documents]\n",
      "\n",
      "dictionary = corpora.Dictionary(texts)\n",
      "corpus = [dictionary.doc2bow(text) for text in texts]\n",
      "\n",
      "#print corpus\n",
      "tfidf = models.TfidfModel(corpus) \n",
      "corpus_tfidf = tfidf[corpus]\n",
      "\n",
      "#lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=100)\n",
      "#lsi.print_topics(20)\n",
      "\n",
      "n_topics = 20\n",
      "lda = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=n_topics)\n",
      "\n",
      "for i in range(0, n_topics):\n",
      "    temp = lda.show_topic(i, 10)\n",
      "    terms = []\n",
      "    for term in temp:\n",
      "        terms.append(term[1])\n",
      "    print \"Top 10 terms for topic #\" + str(i) + \": \"+ \", \".join(terms)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Top 10 terms for topic #0: study, final, left, offensive, hospital, deal;, wiretaps, cavalier, disregard, ex-prosecutor\n",
        "Top 10 terms for topic #1: fears, sympathy, karzai, smoking, step, bill, steel, producers, merger,, france\n",
        "Top 10 terms for topic #2: holds, 1998, services, jousting, faulted, didn't, removed, woman's, tube, congress,\n",
        "Top 10 terms for topic #3: japan, recruiting, voices, bush, doesn't, sending, buy, suspected, using, mourners\n",
        "Top 10 terms for topic #4: widens, h.m.o., hopes, violence, says, vanished, coverage, dividing, kosovo's, unquenched\n",
        "Top 10 terms for topic #5: feeling, baghdad, women, sue, describes, sports,, horror, burial, general, red\n",
        "Top 10 terms for topic #6: terror, blasts, woman, nation, global, say, war:, f.b.i., capital;, rises\n",
        "Top 10 terms for topic #7: remains, disease;, cup, devils, fronts, search, challenged:, stanley, nation, linked\n",
        "Top 10 terms for topic #8: doctors, twist, steinbrenner, california;, decisions, hope, arafat:, suha, blue, bmw\n",
        "Top 10 terms for topic #9: duke, target, problems, elevator, jam?, traffic, senators-only, crumbles, parties, saudi\n",
        "Top 10 terms for topic #10: elderly, attacks:, bush's, relent, pressing, hotels, bombers, 2, overview;, baghdad\n",
        "Top 10 terms for topic #11: pakistan, proposes, coalition, lead, church, police, continues, generals, raids, g.i.\n",
        "Top 10 terms for topic #12: farewell, prison,, militant, struggles, negotiations, hundreds, lack, convention, peru's, a-tests\n",
        "Top 10 terms for topic #13: soldier-statesman, finals;, full, n.b.a., meets, politics:, east, past, disclosed, semi-censure\n",
        "Top 10 terms for topic #14: zero, schiavo, swing, three, talks, ghost, lady;, ground, fire:, case\n",
        "Top 10 terms for topic #15: embrace, warmest, poll, finds, mixed, remain, criticizes, lab, scientific, mrs.\n",
        "Top 10 terms for topic #16: focuses, turns, fence, visit, dream, desparate, stand, surgery, begins, debates,\n",
        "Top 10 terms for topic #17: attacks, glimpse, videos, muted, big, g.o.p.'s, give, win, reno, look\n",
        "Top 10 terms for topic #18: angry, speakers, man, chooses, west, barriers, republic, group, preparing, marines\n",
        "Top 10 terms for topic #19: early, figures, school, timetable, conventions, plan, clash, businesses, researchers, bush\n"
       ]
      }
     ],
     "prompt_number": 123
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
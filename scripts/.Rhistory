})
number_correct = sum(sentiment_labels$BEST_FIT %in% twitter_sample$text.text)
number_correct / sample_size
head(sentiment_labels)
head(twitter_sample)
names(twitter_sample)
number_correct = sum(sentiment_labels$BEST_FIT %in% twitter_sample$label.text)
num_correct
number_correct
number_correct / sample_size
head(sentiment_labels)
head(twitter_sample)
head(twitter_sample$label.text)
head(sentiment_labels$BEST_FIT)
class(sentiment_labels$BEST_FIT)
class(twitter_sample$label_text)
class(twitter_sample$label.text)
sum(sentiment_labels$BEST_FIT %in% twitter_sample$label.text)
table(sentiment_label$label.txt)
table(sentiment_label$BEST_FIT)
table(sentiment_labels$BEST_FIT)
table(twitter_sample$label.txt)
table(twitter_sample$label.text)
sentiment_labels$BEST_FIT %in% twitter_sample$label.text
sentiment_labels$BEST_FIT == twitter_sample$label.text
sum(sentiment_labels$BEST_FIT == twitter_sample$label.text)
number_correct = sum(sentiment_labels$BEST_FIT == twitter_sample$label.text)
number_correct / sample_size
head(twitter_sample)
names(twitter_sample)
dim(twitter_sample)
twitter_sample$text.txt
twitter_sample$text.text
length(twitter_sample$text.text)
summary(twitter_sample$text.text)
apply(twitter_sample$text.text, 1, length)
foo = twitter_sample$text.text
head(foo)
class(foo)
length(foo[1])
length(foo[2])
number_correct = sum(sentiment_labels$BEST_FIT == twitter_sample$label.text)
getwd()
setwd('/Users/rweiss/Dropbox/presentations/MozFest2013/data')
foo = read.csv('twitter_sentiment_output.tsv')
head(foo)
foo = read.csv('twitter_sentiment_output.tsv', sep="\t")
dim(foo)
foo = read.csv('twitter_sentiment_output.tsv', sep="\t", quote="")
dim(foo)
head(foo)
foo = read.csv('twitter_sentiment_output.tsv', sep="\t", quote="", head=F)
head(Foo)
head(foo)
table(foo$V1)
write.csv(twitter_sample)
?write.csv
write.csv(twitter_sample, "twitter_sample.csv")
dim(twitter_sample)
foo = read.csv('twitter_sentiment_output.tsv', sep="\t", quote="")
head(foo)
dim(foo)
foo = read.csv('twitter_sentiment_output.tsv', sep="\t", quote="", header=F, stringAsFactors=F)
foo = read.csv('twitter_sentiment_output.tsv', sep="\t", quote="", header=F)
dim(foo)
foo = read.csv('twitter_sentiment_output.tsv', sep="\t", quote="", header=F, stringsAsFactors, comment.char="")
sample_size = 500
num_documents = dim(twitter_sentiment)[1]
twitter_sample <- twitter_sentiment[sample(1:num_documents,size=sample_size,replace=FALSE),]
dim(twitter_sample)
write.csv(twitter_sample, "twitter_sample.csv")
sample_size = 500
num_documents = dim(twitter_sentiment)[1]
twitter_sample <- twitter_sentiment[sample(1:num_documents,size=sample_size,replace=FALSE),]
dim(twitter_sample)
library(plyr)
library(reshape2)
#setwd("/Users/rweiss/Dropbox/presentations/MozFest2013/scripts")
setwd("/Users/rweiss/Documents/io-2013-slides/scripts/data/")
twitter_sentiment = read.csv("twitter_sentiment.tsv", sep="\t", stringsAsFactors=F)
dim(twitter_sentiment)
sample_size = 500
num_documents = dim(twitter_sentiment)[1]
twitter_sample <- twitter_sentiment[sample(1:num_documents,size=sample_size,replace=FALSE),]
dim(twitter_sample)
?read.csv
?write.csv
write.csv(twitter_sample, "twitter_sample.csv", sep="\t", quote="\"", eol="\n")
write.csv(twitter_sample, "twitter_sample.csv", sep="\t", eol="\n")
write.csv(twitter_sample, "twitter_sample.csv", sep=",", eol="\n")
pwd
getwd()
write.csv(twitter_sample, "twitter_sample.csv", eol="\n")
dim(twitter_sample)
twitter_sample
write.table(twitter_sample, "twitter_sample.csv", eol="\n")
getwd()
write.csv(twitter_sample, "twitter_sample.csv", eol="\n")
write.csv(twitter_sample, "twitter_sample.csv", eol="\n")
write.table(twitter_sample, file = "foo.csv", sep = ",", col.names = NA,
qmethod = "double")
twitter_sentiment[1]
setwd("/Users/rweiss/Dropbox/presentations/MozFest2013/scripts")
twitter_sentiment = read.csv("twitter_sentiment.tsv", sep="\t", stringsAsFactors=F)
setwd("/Users/rweiss/Dropbox/presentations/MozFest2013/data/")
twitter_sentiment = read.csv("twitter_sentiment.tsv", sep="\t", stringsAsFactors=F)
library(sentiment)
sample_size = 500
num_documents = dim(twitter_sentiment)[1]
twitter_sample <- twitter_sentiment[sample(1:num_documents,size=sample_size,replace=FALSE),]
dim(twitter_sample)
write.table(twitter_sample, "twitter_sample.csv", eol="\n")
write.csv(twitter_sample, "twitter_sample.csv")
write.table(twitter_sample, "twitter_sample.csv")
sample_size = 50
num_documents = dim(twitter_sentiment)[1]
twitter_sample <- twitter_sentiment[sample(1:num_documents,size=sample_size,replace=FALSE),]
dim(twitter_sample)
write.table(twitter_sample, "twitter_sample.csv")
?write.table
?write.csv
?write
write(twitter_sample, "twitter_sample.csv")
write(as.matrix(twitter_sample_, "twitter_sample.csv", ncolumns=2)
write(as.matrix(twitter_sample), "twitter_sample.csv", ncolumns=2)
dim(twitter_sample)
twitter_sample
write.table(twitter_sample, "twitter_sample.csv")
twitter_sample[26,]
twitter_sample[27,]
twitter_sample[28,]
twitter_sample[29,]
dim(twitter_sentiment)
setwd("/Users/rweiss/Dropbox/presentations/MozFest2013/data/")
twitter_sentiment = read.csv("twitter_sentiment.tsv", sep="\t", stringsAsFactors=F)
dim(twitter_sentiment)
twitter_sentiment = read.csv("twitter_sentiment.tsv", sep="\t")
twitter_sentiment = read.csv("twitter_sentiment.tsv", sep="\t", header=T)
dim(twitter_sentiment)
twitter_sentiment = read.csv("twitter_sentiment.tsv", header=T)
?read.csv
twitter_sentiment = read.csv("twitter_sentiment.tsv", header=T, sep="\t", quote="")
dim(twitter_sentiment)
library(sentiment)
sample_size = 50
num_documents = dim(twitter_sentiment)[1]
twitter_sample <- twitter_sentiment[sample(1:num_documents,size=sample_size,replace=FALSE),]
dim(twitter_sample)
write.table(twitter_sample, "twitter_sample.csv")
sample_size = 500
num_documents = dim(twitter_sentiment)[1]
twitter_sample <- twitter_sentiment[sample(1:num_documents,size=sample_size,replace=FALSE),]
dim(twitter_sample)
write.table(twitter_sample, "twitter_sample.csv")
class(twitter_sample) #make sure it is a data frame object
head(twitter_sample) # Look at the first six lines or so
#summary(twitter_sample) #summarize the data
sapply(twitter_sample, class) #look at the class of each column
dim(twitter_sample) #Check the dimensions, rows and columns
sentiment_labels = ddply(twitter_sample, .(text.text), function(x){
classify_polarity(x)
})
number_correct = sum(sentiment_labels$BEST_FIT == twitter_sample$label.text)
number_correct / sample_size
library(plyr)
library(reshape2)
class(twitter_sample) #make sure it is a data frame object
head(twitter_sample) # Look at the first six lines or so
#summary(twitter_sample) #summarize the data
sapply(twitter_sample, class) #look at the class of each column
dim(twitter_sample) #Check the dimensions, rows and columns
sentiment_labels = ddply(twitter_sample, .(text.text), function(x){
classify_polarity(x)
})
write.csv(twitter_sample, "twitter_sample.csv", sep="\t")
write.csv(twitter_sample, "twitter_sample.csv")
foo = read.csv('twitter_sentiment_output.tsv', sep="\t", quote="", header=F)
number_correct / sample_size
number_correct = sum(sentiment_labels$BEST_FIT == twitter_sample$label.text)
number_correct / sample_size
twitter_sample$label_text
twitter_sample$label.text
sentiment_labels$BEST_FIT
?classify_polarity
class(twitter_sample)
sentiment_labels = classify_polarity(twitter_sample$text.text, algorithm="bayes")
head(sentiment_labels)
sentiment_labels$BEST_FIT
class(sentiment_labels)
sentiment_labels[,4]
sentiment_labels[,4] %in% twitter_sample$label.txt
sentiment_labels[,4] == twitter_sample$label.txt
sentiment_labels[,4] == lower(twitter_sample$label.txt)
sentiment_labels[,4] == tolower(twitter_sample$label.txt)
tolower(twitter_sample$label.txt)
tolower(twitter_sample$label.text)
sentiment_labels[,4] == tolower(twitter_sample$label.text)
sum(sentiment_labels[,4] == tolower(twitter_sample$label.text))
sum(sentiment_labels[,4] == tolower(twitter_sample$label.text))/sample_size
twitter_dtm <- create_matrix(as.vector(twitter_sample$Title),
language="english",
removeNumbers=TRUE,
stemWords=TRUE,
weighting=weightTfIdf)
twitter_dtm <- create_matrix(as.vector(twitter_sample$text.text),
language="english",
removeNumbers=TRUE,
stemWords=TRUE,
weighting=weightTfIdf)
twitter_dtm
train_n = sample_size * 0.75
test_n = sample_size * 0.25
corpus <- create_container(twitter_dtm,
twitter_sample$label.text,
trainSize=1:train_n,
testSize=(train_n+1):sample_size,
virgin=FALSE)
library(RTextTools)
corpus <- create_container(twitter_dtm,
twitter_sample$label.text,
trainSize=1:train_n,
testSize=(train_n+1):sample_size,
virgin=FALSE)
names(attributes(corpus)) #class matrix_container
paste(twitter_sample[1,]$text.text)
corpus@column_names[corpus@training_matrix[1]@ja]
twitter_sample
corpus <- create_container(twitter_dtm,
twitter_sample$label.text,
trainSize=1:train_n,
testSize=(train_n+1):sample_size,
virgin=FALSE)
corpus
names(attributes(corpus)) #class matrix_container
paste(twitter_sample[1,]$text.text)
paste(twitter_sample[1:10,]$text.text)
corpus@column_names[corpus@training_matrix[1:10]@ja]
head(corpus@column_names)
corpus@training_matrix[1:10]@ja
corpus@training_matrix[1]@ja
corpus@training_matrix[10]@ja
corpus@training_matrix[10]
str(corpus)
twitter_dtm
train_n
test_n
corpus@training_matrix[10]@ia
head(Foo)
head(foo)
table(foo$v1)
table(foo$V1)
table(sentiment_labels)
str(sentiment_labels)
table(sentiment_labels$BEST_FIT)
?classify_polarity
class(twitter_sample$text.text)
sentiment_labels = classify_polarity(as.vector(twitter_sample$text.text), algorithm="bayes")
str(sentiment_labels)
class(sentiment_labels)
dim(sentiment_labels)
table(sentiment_labels[,4])
table(foo$V1)
table(twitter_sample$label.text)
number_correct / sample_size
number_correct = sum(sentiment_labels$BEST_FIT == twitter_sample$label.text)
number_correct / sample_size
sample_size = 1000
num_documents = dim(twitter_sentiment)[1]
twitter_sample <- twitter_sentiment[sample(1:num_documents,size=sample_size,replace=FALSE),]
dim(twitter_sample)
write.csv(twitter_sample, "twitter_sample.csv")
class(twitter_sample) #make sure it is a data frame object
head(twitter_sample) # Look at the first six lines or so
#summary(twitter_sample) #summarize the data
sapply(twitter_sample, class) #look at the class of each column
dim(twitter_sample) #Check the dimensions, rows and columns
#sentiment_labels = ddply(twitter_sample, .(text.text), function(x){
#  classify_polarity(x, algorithm="bayes")
#})
sentiment_labels = classify_polarity(as.vector(twitter_sample$text.text), algorithm="bayes")
number_correct = sum(sentiment_labels$BEST_FIT == twitter_sample$label.text)
class(sentiment_labels)
sentiment_labels = ddply(twitter_sample, .(text.text), function(x){
classify_polarity(x, algorithm="bayes")
})
number_correct = sum(sentiment_labels$BEST_FIT == twitter_sample$label.text)
number_correct / sample_size
head(foo)
number_correct = sum(foo$V1 == twitter_sample$label.text)
table(foo$V1)
levels(foo$V1)
levels(twitter_sample$label.text)
levels(twitter_sample$label.text) = c(twitter_sample$label.text, "unsure")
number_correct = sum(foo$V1 == twitter_sample$label.text)
number_correct / sample_size
tweets <- read.delim("debate08_sentiment_tweets.tsv",
comment.char="#",
colClasses=c("numeric", "character", "character", "character",
"character", "factor", "factor", "factor", "factor",
"factor", "factor", "factor", "factor"))
getwd()
gc()
tweets[1,]
rm(list=ls(all=T))
gc()
gc()
gc()
gc()
ls()
reviews_sample$content = factor(reviews_sample$content)
library(sentiment)
rt_neg = read.delim("rt-polaritydata/rt-polaritydata/rt-polarity.neg", header=F)
rt_pos = read.delim("rt-polaritydata/rt-polaritydata/rt-polarity.pos", header=F)
rt_neg = data.frame(rt_neg)
rt_pos = data.frame(rt_pos)
rt_neg$label = "negative"
rt_pos$label = "positive"
reviews = rbind(rt_neg, rt_pos)
names(reviews) = c('content','label')
#let's start off with a small sample
sample_size = 100
num_documents = dim(reviews)[1]
reviews_sample <- reviews[sample(1:num_documents,size=sample_size,replace=FALSE),]
reviews_sample$content = factor(reviews_sample$content)
library(plyr)
library(reshape2)
setwd("/Users/rweiss/Dropbox/presentations/MozFest2013/data/")
rt_neg = read.delim("rt-polaritydata/rt-polaritydata/rt-polarity.neg", header=F)
rt_pos = read.delim("rt-polaritydata/rt-polaritydata/rt-polarity.pos", header=F)
rt_neg = data.frame(rt_neg)
rt_pos = data.frame(rt_pos)
rt_neg$label = "negative"
rt_pos$label = "positive"
reviews = rbind(rt_neg, rt_pos)
names(reviews) = c('content','label')
#let's start off with a small sample
sample_size = 100
num_documents = dim(reviews)[1]
reviews_sample <- reviews[sample(1:num_documents,size=sample_size,replace=FALSE),]
reviews_sample$content = factor(reviews_sample$content)
foo = data.frame(reviews_sample$label, reviews_sample$content)
write.csv(foo, 'reviews_sample7.csv', row.names=F)
dim(reviews_sample)
write.csv(foo, 'reviews_sample7.csv', row.names=F, quote="")
?write.csv
write.csv(foo, 'reviews_sample7.csv', row.names=F, quote=F)
dim(foo)
class(foo)
sum(levels(foo$content))
levels(foo$content)
head(foo)
names(foo)
class(foo$reviews_sample.content)
class(foo$reviews_sample.label)
foo = data.frame(as.numeric(reviews_sample$label), as.character(reviews_sample$content))
head(foo)
head(reviews_sample)
foo = data.frame(as.character(reviews_sample$label), as.character(reviews_sample$content))
write.csv(foo, 'reviews_sample7.csv', row.names=F, quote=F)
reviews_samples
review_samples
reviews_sample
table(review_sample$label)
table(reviews_sample$label)
levels(reviews_sample$label)
class(reviews_sample$label)
reviews_sample$content = as.character(reviews_sample$content)
sapply(reviews_sample, class)
foo = data.frame(as.character(reviews_sample$label), as.character(reviews_sample$content))
class(foo)
write.csv(foo, 'reviews_sample7.csv', row.names=F, quote=F)
write.csv(foo, 'reviews_sample7.csv', row.names=F, quote=F, sep="\t")
write.table(foo, 'reviews_sample7.csv', row.names=F, quote=F, sep="\t")
getwd()
write.table(foo, 'reviews_sample7.csv', row.names=F, quote=F, sep="\t")
rt_neg = read.delim("rt-polaritydata/rt-polaritydata/rt-polarity.neg", header=F, quote="")
rt_pos = read.delim("rt-polaritydata/rt-polaritydata/rt-polarity.pos", header=F, quote="")
rt_neg = data.frame(rt_neg)
rt_pos = data.frame(rt_pos)
rt_neg$label = "negative"
rt_pos$label = "positive"
reviews = rbind(rt_neg, rt_pos)
names(reviews) = c('content','label')
sample_size = 100
num_documents = dim(reviews)[1]
reviews_sample <- reviews[sample(1:num_documents,size=sample_size,replace=FALSE),]
reviews_sample$content = as.character(reviews_sample$content)
dim(reviews_sampel)
dim(reviews_sample)
foo = data.frame(as.character(reviews_sample$label), as.character(reviews_sample$content))
write.table(foo, 'reviews_sample7.csv', row.names=F, quote=F, sep="\t")
write.table(foo, 'reviews_sample.csv', row.names=F, quote=F, sep="\t")
lda
library(RTextTools)
data(NYTimes)
dim(NYTimes)
head(NYTimes)
valid = c(3, 12, 15, 16, 19, 20, 29)
NYTimes = NYTimes[NYTimes$Topic.Code %in% valid,]
#consider only using 3, 12, 15, 16, 19, 20, 29
num_documents = dim(NYTimes)[1]
#Examine the data
class(NYTimes) #make sure it is a data frame object
head(NYTimes) # Look at the first six lines or so
summary(NYTimes) #summarize the data
sapply(NYTimes, class) #look at the class of each column
dim(NYTimes) #Check the dimensions, rows and columns
# [OPTIONAL] SUBSET YOUR DATA TO GET A RANDOM SAMPLE
#sample_size = 500
sample_size = num_documents
NYT_sample <- NYTimes[sample(1:num_documents,size=sample_size,replace=FALSE),]
out_data = data.frame(NYT_sample$Topic.Code, NYT_sample$Title)
write.csv(out_data, 'nyt_title_data.csv', row.names=F)
write.csv(reviews_sample$content, 'reviews_data.csv', row.names=F, col.names=F)
write.csv(foo, 'reviews_sample.csv', row.names=F, quote=F)
write.csv(foo, 'reviews_sample.csv', row.names=F, quote=F, sep="\t")
write.table(foo, 'reviews_sample.csv', row.names=F, quote=F, sep="\t")
write.table(foo, 'reviews_sample.csv', col.names=c('label','content'), row.names=F, quote=F, sep="\t")
setwd("/Users/rweiss/Dropbox/presentations/MozFest2013/data/")
rt_neg = read.delim("rt-polaritydata/rt-polaritydata/rt-polarity.neg", header=F, quote="")
rt_pos = read.delim("rt-polaritydata/rt-polaritydata/rt-polarity.pos", header=F, quote="")
table(NYTimes$Topic.Code)
data(NYTimes)
table(NYTimes$Topic.Code)
table(NYTimes$Topic.Code)
#consider only using 3, 12, 15, 16, 19, 20, 29
valid = c(3, 12, 15, 16, 19, 20, 29)
NYTimes = NYTimes[NYTimes$Topic.Code %in% valid,]
table(NYTimes$Topic.Code)
num_documents = dim(NYTimes)[1]
#Examine the data
class(NYTimes) #make sure it is a data frame object
head(NYTimes) # Look at the first six lines or so
summary(NYTimes) #summarize the data
sapply(NYTimes, class) #look at the class of each column
dim(NYTimes) #Check the dimensions, rows and columns
# [OPTIONAL] SUBSET YOUR DATA TO GET A RANDOM SAMPLE
#sample_size = 500
sample_size = num_documents
NYT_sample <- NYTimes[sample(1:num_documents,size=sample_size,replace=FALSE),]
out_data = data.frame(NYT_sample$Topic.Code, NYT_sample$Title)
write.csv(out_data, 'nyt_title_data.csv', row.names=F)
# CREATE A TERM-DOCUMENT MATRIX THAT REPRESENTS WORD FREQUENCIES IN EACH DOCUMENT
# WE WILL TRAIN ON THE Title COLUMNS
#NYT_dtm <- create_matrix(data.frame(NYT_sample$Title,NYT_sample$Subject),
NYT_dtm <- create_matrix(as.vector(NYT_sample$Title),
language="english",
removeNumbers=TRUE,
stemWords=TRUE,
weighting=weightTfIdf)
NYT_dtm # Sparse Matrix object
library(ggplot2)
train_n = round(sample_size * 0.8)
test_n = round(sample_size * 0.2)
corpus <- create_container(NYT_dtm,
NYT_sample$Topic.Code,
trainSize=1:train_n,
testSize=(train_n+1):sample_size,
virgin=FALSE)
names(attributes(corpus))
paste(NYT_sample[1,]$Title) # original data
corpus@column_names[corpus@training_matrix[1]@ja] # preprocessed data
# TRAIN MODELS
models <- train_models(corpus, algorithms=c("SVM","MAXENT"))
results <- classify_models(corpus, models)
analytics <- create_analytics(corpus, results)
nyt_codes = read.csv("../data/nytimes_codes.csv")
test_start_index = num_documents - train_n
svm_full = data.frame(NYT_sample[1730:2161,]$Title, results$SVM_LABEL)
maxent_full = data.frame(NYT_sample[1730:2161,]$Title, results$MAXENTROPY_LABEL)
names(svm_full) = c('content','code')
names(maxent_full) = c('content','code')
svm_full= merge(svm_full, nyt_codes)
maxent_full= merge(maxent_full, nyt_codes)
x <- as.numeric(rownames(analytics@algorithm_summary))[-20]
y <- analytics@algorithm_summary$MAXENT_RECALL[-20]
qplot(x, y, type="l", lwd=3, main="Maximum Entropy Topic Accuracy", ylab="Recall Accuracy", xlab="Topic")
qplot(x, y)
analytics@algorithm_summary
x <- as.numeric(rownames(analytics@algorithm_summary))[-20]
y <- analytics@algorithm_summary$SVM_RECALL[-20]
plot(x, y, type="l", lwd=3, main="Support Vector Machine Topic Accuracy", ylab="Recall Accuracy", xlab="Topic")
abline(h=.75, lwd=2, col="maroon")
text(x, y, adj=1.2)
x <- as.numeric(rownames(analytics@algorithm_summary))[-20]
y <- analytics@algorithm_summary$MAXENT_RECALL[-20]
plot(x, y, type="l", lwd=3, main="Maximum Entropy Topic Accuracy", ylab="Recall Accuracy", xlab="Topic")
abline(h=.75, lwd=2, col="maroon")
text(x, y, adj=1.2)
rownames(analytics@algorithm_summary)
x <- as.character(rownames(analytics@algorithm_summary))[-20]
y <- analytics@algorithm_summary$SVM_RECALL[-20]
plot(x, y, type="l", lwd=3, main="Support Vector Machine Topic Accuracy", ylab="Recall Accuracy", xlab="Topic")
abline(h=.75, lwd=2, col="maroon")
text(x, y, adj=1.2)
as.character(rownames(analytics@algorithm_summary))[-20]
analytics@algorithm_summary$SVM_RECALL[-20]
analytics@algorithm_summary$MAXENT_RECALL[-20]
analytics@algorithm_summary$MAXENT_RECALL
analytics@algorithm_summary
analytics@algorithm_summary$MAXENTROPY_RECALL
x <- as.character(rownames(analytics@algorithm_summary))[-20]
y <- analytics@algorithm_summary$MAXENTROPY_RECALL[-20]
plot(x, y, type="l", lwd=3, main="Maximum Entropy Topic Accuracy", ylab="Recall Accuracy", xlab="Topic")
abline(h=.75, lwd=2, col="maroon")
text(x, y, adj=1.2)
x <- as.character(rownames(analytics@algorithm_summary))[-20]
y <- analytics@algorithm_summary$SVM_RECALL[-20]
plot(x, y, type="l", lwd=3, main="Support Vector Machine Topic Accuracy", ylab="Recall Accuracy", xlab="Topic")
abline(h=.75, lwd=2, col="maroon")
text(x, y, adj=1.2)
lda
